{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gjQGXlVA1gmW",
        "outputId": "df22561d-b641-42d2-eafe-b55f2012483d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.37.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.2.2)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (16.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2024.6.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.26.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.16.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install ray[tune]\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ImLHI6XFyI4J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "from ray import tune\n",
        "from ray import train\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune import CLIReporter\n",
        "import psutil\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_names: List[str] = [\n",
        "  \"microsoft/DialoGPT-small\",\n",
        "  # \"microsoft/DialoGPT-medium\"\n",
        "  ]\n",
        "\n",
        "# each model is imported (pre trained version), as well as the tokenizers (model-specific)\n",
        "models: Dict[str, AutoModelForCausalLM] = {name: AutoModelForCausalLM.from_pretrained(name) for name in model_names}\n",
        "tokenizers: Dict[str, AutoTokenizer] = {name: AutoTokenizer.from_pretrained(name) for name in model_names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AhvGiTeXdam"
      },
      "outputs": [],
      "source": [
        "# sometimes the pad_token is not included in tokenizer (end of sequence token is chosen)\n",
        "for name, tokenizer in tokenizers.items():\n",
        "  if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpEkL7GpZIGu"
      },
      "outputs": [],
      "source": [
        "# here, the real conversations will be loaded\n",
        "conversations: List[Dict[str, str]] = [\n",
        "  {\"input\": \"Hello\", \"output\": \"Hello, can I help you?\"}\n",
        "]\n",
        "\n",
        "conversations_val: List[Dict[str, str]] = [\n",
        "  {\"input\": \"Good morning\", \"output\": \"Good morning, can I help you?\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(tokenizer: AutoTokenizer, conversations: List[Dict[str, str]]) -> int:\n",
        "    lengths = []\n",
        "    for conversation in conversations:\n",
        "        input_length = len(tokenizer(conversation[\"input\"], truncation=False)[\"input_ids\"])\n",
        "        output_length = len(tokenizer(conversation[\"output\"], truncation=False)[\"input_ids\"])\n",
        "        lengths.append(max(input_length, output_length))\n",
        "    return max(lengths)\n",
        "\n",
        "max_length = get_max_length(tokenizer, conversaciones + conversaciones_val)"
      ],
      "metadata": {
        "id": "M-O_lO2ELfbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkhnbngu3vOn"
      },
      "outputs": [],
      "source": [
        "# depending on the model expected input, the way the conversations are tokenized is different (labels is what we are trying to predict)\n",
        "def preprocess_conversations(tokenizer: AutoTokenizer, conversations: List[Dict[str, str]], max_length: int) -> Dataset:\n",
        "  input_texts: List[str] = []\n",
        "  target_texts: List[str] = []\n",
        "\n",
        "  for conversation in conversations:\n",
        "    input_texts.append(conversation[\"input\"])\n",
        "    target_texts.append(conversation[\"output\"])\n",
        "\n",
        "  tokenized_inputs: Dict[str, torch.Tensor] = tokenizer(input_texts, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "  tokenized_outputs: Dict[str, torch.Tensor] = tokenizer(target_texts, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  dataset: Dataset = Dataset.from_dict({\n",
        "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
        "    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
        "    \"labels\": tokenized_outputs[\"input_ids\"]\n",
        "  })\n",
        "\n",
        "  return dataset\n",
        "\n",
        "tokenized_datasets: Dict[str, Dataset] = {name: preprocess_conversations(tokenizers[name], conversations, max_length) for name in model_names}\n",
        "tokenized_datasets_val: Dict[str, Dataset] = {name: preprocess_conversations(tokenizers[name], conversations_val, max_length) for name in model_names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy4N832yfT5J"
      },
      "outputs": [],
      "source": [
        "# a custom trainer method is needed because sometimes the tensors are stored in different physical devices, so cloning the weights is needed\n",
        "# before saving the model\n",
        "class CustomTrainer(Trainer):\n",
        "  def _save(self, output_dir: str, state_dict: Optional[Dict[str, torch.Tensor]]=None) -> None:\n",
        "    if hasattr(self.model, \"base_model\"):\n",
        "      if hasattr(self.model.base_model, \"lm_head\") and hasattr(self.model.base_model, \"transformer\"):\n",
        "        if self.model.base_model.lm_head.weight is self.model.base_model.transformer.wte.weight:\n",
        "          self.model.base_model.lm_head.weight = torch.nn.Parameter(self.model.base_model.lm_head.weight.clone())\n",
        "\n",
        "    super()._save(output_dir, state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFFE5nf9e-bd"
      },
      "outputs": [],
      "source": [
        "# custom training model\n",
        "def train_model(model: nn.Module, custom_name:str, tokenizer: AutoTokenizer, tokenized_dataset: Dataset, tokenized_dataset_val: Dataset) -> None:\n",
        "  # Configurar los argumentos del entrenamiento\n",
        "  training_args: TrainingArguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,  # each epoch has (len(conversations) / batch_size) steps\n",
        "    per_device_train_batch_size=1,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1,\n",
        "    save_steps=1,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    report_to=\"tensorboard\"\n",
        "  )\n",
        "\n",
        "  # data collator to apply pad token to input sequences\n",
        "  data_collator: DataCollatorWithPadding = DataCollatorWithPadding(tokenizer=tokenizer) # type: ignore\n",
        "  trainer: Trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  # store model under folder with the name\n",
        "  trainer.save_model(f\"./results/{custom_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6knaIfbYyNK"
      },
      "outputs": [],
      "source": [
        "class CustomTransformerModel(nn.Module):\n",
        "  def __init__(self, base_model_name: str, custom_layer: bool, num_heads: int, num_layers: int) -> None:\n",
        "    super(CustomTransformerModel, self).__init__()\n",
        "    self.base_model: AutoModelForCausalLM  = AutoModelForCausalLM.from_pretrained(base_model_name, output_hidden_states=True)\n",
        "    self.custom_layer: bool = custom_layer\n",
        "    if self.custom_layer:\n",
        "      self.d_model: int = self.base_model.config.hidden_size\n",
        "      self.transformer_encoder: nn.TransformerEncoder = nn.TransformerEncoder(\n",
        "        nn.TransformerEncoderLayer(nhead=num_heads, d_model=self.d_model, batch_first=True),\n",
        "        num_layers=num_layers)\n",
        "      self.fc: nn.Linear = nn.Linear(self.d_model, self.base_model.config.vocab_size)\n",
        "\n",
        "  def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    # feed the input to the base model\n",
        "    outputs: torch.Tensor = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # obtain outputs from base model\n",
        "    logits: torch.Tensor = None\n",
        "    if self.custom_layer:\n",
        "      hidden_states: torch.Tensor = outputs.hidden_states[-1]\n",
        "      # feed base model outputs to additional layer\n",
        "      transformer_output: torch.Tensor = self.transformer_encoder(hidden_states.permute(1, 0, 2))\n",
        "      output: torch.Tensor = transformer_output.permute(1, 0, 2)\n",
        "      logits = self.fc(output)  # feed the output to extra linear layer\n",
        "    else:\n",
        "      logits = outputs.logits\n",
        "    # compute loss (cross entropy)\n",
        "    loss_fct: nn.CrossEntropyLoss = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    # flatten logits and labels\n",
        "    logits = logits.view(-1, self.base_model.config.vocab_size)\n",
        "    labels = labels.view(-1)\n",
        "    loss: torch.Tensor = loss_fct(logits, labels)\n",
        "    return (loss, logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfLh0o7ayP7T"
      },
      "outputs": [],
      "source": [
        "def search_space() -> Dict[str, Any]:\n",
        "  return {\n",
        "    \"num_heads\": tune.choice([1]),\n",
        "    \"num_layers\": tune.choice([1]),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKEOj3BBbulg"
      },
      "outputs": [],
      "source": [
        "def evaluate_hyperparameters(config: Dict[str, Any], model_name: str, custom_layer: bool) -> None:\n",
        "  # create model with current hyperparameter configuration\n",
        "  num_heads: int = config[\"num_heads\"]\n",
        "  num_layers: int = config[\"num_layers\"]\n",
        "\n",
        "  model: CustomTransformerModel  = CustomTransformerModel(model_name, custom_layer, num_heads=num_heads, num_layers=num_layers)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # train model\n",
        "  custom_name: str = model_name\n",
        "  if custom_layer:\n",
        "    custom_name += \"_custom_layer\" + \"_num_heads_\" + str(num_heads) + \"_num_layers_\" + str(num_layers)\n",
        "  else:\n",
        "    custom_name += \"_base_model\"\n",
        "  train_model(model, custom_name, tokenizers[model_name], tokenized_datasets[model_name], tokenized_datasets_val[model_name])\n",
        "\n",
        "  # compute loss on validation set\n",
        "  validation_data = tokenized_datasets_val[model_name]\n",
        "\n",
        "  # feed validation data to model\n",
        "  with torch.no_grad():\n",
        "    input_ids: torch.Tensor = torch.tensor(validation_data[\"input_ids\"]).to(device)  # Convertir a tensor\n",
        "    attention_mask: torch.Tensor = torch.tensor(validation_data[\"attention_mask\"]).to(device)  # Convertir a tensor\n",
        "    labels: torch.Tensor = torch.tensor(validation_data[\"labels\"]).to(device)  # Convertir a tensor\n",
        "\n",
        "    outputs: Tuple[torch.Tensor, torch.Tensor] = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      labels=labels\n",
        "    )\n",
        "\n",
        "  # flatten logits and labels to compute loss and accuracy\n",
        "  logits: torch.Tensor = outputs[1].view(-1, outputs[1].size(-1))\n",
        "  labels = labels.view(-1)\n",
        "\n",
        "  loss_fct: nn.CrossEntropyLoss = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "  validation_loss: torch.Tensor = loss_fct(logits, labels)\n",
        "\n",
        "  # report metrics to Ray Tune\n",
        "  train.report({\"loss\": validation_loss.item()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGWcjM-Uybx-"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model_name: str, custom_layer: bool) -> Any:\n",
        "  # define search space\n",
        "  search_config: Dict[str, Any] = search_space()\n",
        "\n",
        "  # define scheduler\n",
        "  scheduler: ASHAScheduler = ASHAScheduler(\n",
        "    metric=\"loss\",\n",
        "    mode=\"min\",\n",
        "    max_t=1,  # max epochs number\n",
        "    grace_period=1,  # min epochs number before halting trial\n",
        "    reduction_factor=2\n",
        "  )\n",
        "\n",
        "  # log results during tuning\n",
        "  reporter: CLIReporter = CLIReporter(\n",
        "    metric_columns=[\"loss\", \"training_iteration\"]\n",
        "  )\n",
        "\n",
        "  # execute actual search\n",
        "  result: Any = tune.run(\n",
        "    tune.with_parameters(evaluate_hyperparameters, model_name=model_name, custom_layer=custom_layer),\n",
        "    config=search_config,\n",
        "    num_samples=1,  # number of hyperparameter configuration to try\n",
        "    scheduler=scheduler,\n",
        "    progress_reporter=reporter,\n",
        "    resources_per_trial={\"cpu\": psutil.cpu_count(logical=True), \"gpu\": torch.cuda.device_count()}  # use max number of cpus and gpus\n",
        "  )\n",
        "\n",
        "  # obtain best results\n",
        "  best_trial: Any = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "  print(f\"Best configuration: {best_trial.config}\")\n",
        "  print(f\"Best loss: {best_trial.last_result['loss']}\")\n",
        "  return best_trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1DTi8sI4ZZoA"
      },
      "outputs": [],
      "source": [
        "def train_models(custom_layer: bool) -> Dict[str, float]:\n",
        "  trained_models: Dict[str, float] = {}\n",
        "\n",
        "  for model_name in model_names:\n",
        "\n",
        "    best_trial: Any = tune_hyperparameters(model_name, custom_layer)\n",
        "\n",
        "    num_heads: int = best_trial.config[\"num_heads\"]\n",
        "    num_layers: int = best_trial.config[\"num_layers\"]\n",
        "    custom_name: str = model_name\n",
        "    if custom_layer:\n",
        "      custom_name += \"_custom_layer\" + \"_num_heads_\" + str(num_heads) + \"_num_layers_\" + str(num_layers)\n",
        "    else:\n",
        "      custom_name += \"_base_model\"\n",
        "\n",
        "    # train model with best hyperparameter configuration\n",
        "    model: CustomTransformerModel = CustomTransformerModel(\n",
        "      model_name,\n",
        "      custom_layer,\n",
        "      num_heads=num_heads,\n",
        "      num_layers=num_layers\n",
        "    )\n",
        "\n",
        "    # during the hyperparameter search, the model were already saved??\n",
        "    train_model(model, custom_name, tokenizers[name], tokenized_datasets[name], tokenized_datasets_val[model_name])\n",
        "\n",
        "    trained_models[custom_name] = (best_trial.last_result['loss'])  # type: ignore\n",
        "  return trained_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi47KLEV0yCS"
      },
      "outputs": [],
      "source": [
        "def main_trainer() -> Dict[str, float]:\n",
        "  return train_models(custom_layer=False) | train_models(custom_layer=True) # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "2FD-ilVYeNEH",
        "outputId": "b2b97e2d-d948-4227-bdb2-c036b5c0154a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-17 10:43:19,849\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------+\n",
            "| Configuration for experiment     evaluate_hyperparameters_2024-10-17_10-43-19   |\n",
            "+---------------------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator                          |\n",
            "| Scheduler                        AsyncHyperBandScheduler                        |\n",
            "| Number of trials                 1                                              |\n",
            "+---------------------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/evaluate_hyperparameters_2024-10-17_10-43-19\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-10-17_10-41-16_855805_3146/artifacts/2024-10-17_10-43-19/evaluate_hyperparameters_2024-10-17_10-43-19/driver_artifacts`\n",
            "\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-10-17 10:43:20. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------+\n",
            "| Trial name                             status       num_heads     num_layers |\n",
            "+------------------------------------------------------------------------------+\n",
            "| evaluate_hyperparameters_9fe56_00000   PENDING              1              1 |\n",
            "+------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=4359)\u001b[0m 2024-10-17 10:43:25.970514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=4359)\u001b[0m 2024-10-17 10:43:25.991870: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=4359)\u001b[0m 2024-10-17 10:43:25.997960: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=4359)\u001b[0m 2024-10-17 10:43:27.151458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial evaluate_hyperparameters_9fe56_00000 started with configuration:\n",
            "+----------------------------------------------------------+\n",
            "| Trial evaluate_hyperparameters_9fe56_00000 config        |\n",
            "+----------------------------------------------------------+\n",
            "| num_heads                                              1 |\n",
            "| num_layers                                             1 |\n",
            "+----------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4359)\u001b[0m {'loss': 8.9861, 'grad_norm': 33.80056381225586, 'learning_rate': 0.0, 'epoch': 1.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4359)\u001b[0m \r                                             \r\r100%|██████████| 1/1 [00:11<00:00,  2.08s/it]\r100%|██████████| 1/1 [00:11<00:00, 11.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4359)\u001b[0m {'train_runtime': 11.9087, 'train_samples_per_second': 0.084, 'train_steps_per_second': 0.084, 'train_loss': 8.986088752746582, 'epoch': 1.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-17 10:43:46,601\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/evaluate_hyperparameters_2024-10-17_10-43-19' in 0.0099s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial evaluate_hyperparameters_9fe56_00000 finished iteration 1 at 2024-10-17 10:43:46. Total running time: 26s\n",
            "+---------------------------------------------------------------+\n",
            "| Trial evaluate_hyperparameters_9fe56_00000 result             |\n",
            "+---------------------------------------------------------------+\n",
            "| checkpoint_dir_name                                           |\n",
            "| time_this_iter_s                                      17.9076 |\n",
            "| time_total_s                                          17.9076 |\n",
            "| training_iteration                                          1 |\n",
            "| loss                                                  6.90507 |\n",
            "+---------------------------------------------------------------+\n",
            "\n",
            "Trial evaluate_hyperparameters_9fe56_00000 completed after 1 iterations at 2024-10-17 10:43:46. Total running time: 26s\n",
            "\n",
            "Trial status: 1 TERMINATED\n",
            "Current time: 2024-10-17 10:43:46. Total running time: 26s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                             status         num_heads     num_layers     iter     total time (s)      loss |\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "| evaluate_hyperparameters_9fe56_00000   TERMINATED             1              1        1            17.9076   6.90507 |\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Best configuration: {'num_heads': 1, 'num_layers': 1}\n",
            "Best loss: 6.905068397521973\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>8.986100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-17 10:44:05,216\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------+\n",
            "| Configuration for experiment     evaluate_hyperparameters_2024-10-17_10-44-05   |\n",
            "+---------------------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator                          |\n",
            "| Scheduler                        AsyncHyperBandScheduler                        |\n",
            "| Number of trials                 1                                              |\n",
            "+---------------------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/evaluate_hyperparameters_2024-10-17_10-44-05\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-10-17_10-41-16_855805_3146/artifacts/2024-10-17_10-44-05/evaluate_hyperparameters_2024-10-17_10-44-05/driver_artifacts`\n",
            "\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2024-10-17 10:44:05. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------+\n",
            "| Trial name                             status       num_heads     num_layers |\n",
            "+------------------------------------------------------------------------------+\n",
            "| evaluate_hyperparameters_baeff_00000   PENDING              1              1 |\n",
            "+------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(pid=4638)\u001b[0m 2024-10-17 10:44:11.190964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=4638)\u001b[0m 2024-10-17 10:44:11.210936: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=4638)\u001b[0m 2024-10-17 10:44:11.217003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=4638)\u001b[0m 2024-10-17 10:44:15.080995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial evaluate_hyperparameters_baeff_00000 started with configuration:\n",
            "+----------------------------------------------------------+\n",
            "| Trial evaluate_hyperparameters_baeff_00000 config        |\n",
            "+----------------------------------------------------------+\n",
            "| num_heads                                              1 |\n",
            "| num_layers                                             1 |\n",
            "+----------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4638)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "\u001b[36m(evaluate_hyperparameters pid=4638)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4638)\u001b[0m {'loss': 12.3478, 'grad_norm': 30.250720977783203, 'learning_rate': 0.0, 'epoch': 1.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4638)\u001b[0m \r                                             \r\r100%|██████████| 1/1 [00:10<00:00,  1.27s/it]\r100%|██████████| 1/1 [00:10<00:00, 10.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(evaluate_hyperparameters pid=4638)\u001b[0m {'train_runtime': 10.6171, 'train_samples_per_second': 0.094, 'train_steps_per_second': 0.094, 'train_loss': 12.347847938537598, 'epoch': 1.0}\n",
            "\n",
            "Trial status: 1 RUNNING\n",
            "Current time: 2024-10-17 10:44:35. Total running time: 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------+\n",
            "| Trial name                             status       num_heads     num_layers |\n",
            "+------------------------------------------------------------------------------+\n",
            "| evaluate_hyperparameters_baeff_00000   RUNNING              1              1 |\n",
            "+------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-17 10:44:40,420\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/evaluate_hyperparameters_2024-10-17_10-44-05' in 0.0047s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial evaluate_hyperparameters_baeff_00000 finished iteration 1 at 2024-10-17 10:44:40. Total running time: 35s\n",
            "+---------------------------------------------------------------+\n",
            "| Trial evaluate_hyperparameters_baeff_00000 result             |\n",
            "+---------------------------------------------------------------+\n",
            "| checkpoint_dir_name                                           |\n",
            "| time_this_iter_s                                      20.6418 |\n",
            "| time_total_s                                          20.6418 |\n",
            "| training_iteration                                          1 |\n",
            "| loss                                                  11.6834 |\n",
            "+---------------------------------------------------------------+\n",
            "\n",
            "Trial evaluate_hyperparameters_baeff_00000 completed after 1 iterations at 2024-10-17 10:44:40. Total running time: 35s\n",
            "\n",
            "Trial status: 1 TERMINATED\n",
            "Current time: 2024-10-17 10:44:40. Total running time: 35s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                             status         num_heads     num_layers     iter     total time (s)      loss |\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "| evaluate_hyperparameters_baeff_00000   TERMINATED             1              1        1            20.6418   11.6834 |\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Best configuration: {'num_heads': 1, 'num_layers': 1}\n",
            "Best loss: 11.683367729187012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:09, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>10.817800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "comparison_dict: Dict[str, float] = main_trainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj9Pt6zT_WKB",
        "outputId": "1b0828e0-8f3d-468d-f48b-f36d6e96ea37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: microsoft/DialoGPT-small_base_model, Loss (cross entropy): 6.905068397521973\n",
            "Model: microsoft/DialoGPT-small_custom_layer_num_heads_1_num_layers_1, Loss (cross entropy): 11.683367729187012\n"
          ]
        }
      ],
      "source": [
        "for model_name, loss in comparison_dict.items():\n",
        "  print(f\"Model: {model_name}, Loss (cross entropy): {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7SZmwGw4knv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972d5453-ffdb-4283-afa5-5e1e65214ef7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results/\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R2L9Y048y_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "74b0f59a-3374-46b2-a80e-dfd580ca315f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = \"emre/spanish-dialoGPT\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "conversaciones = [\n",
        "    {\"input\": \"¡Hola!\", \"output\": \"Hola, ¿cómo puedo ayudarte?\"},\n",
        "]\n",
        "\n",
        "conversaciones_val = [\n",
        "    {\"input\": \"¡Buenos días!\", \"output\": \"Buenos días, ¿cómo puedo ayudarte?\"},\n",
        "]\n",
        "\n",
        "max_length = get_max_length(tokenizer, conversaciones + conversaciones_val)\n",
        "\n",
        "train_dataset = preprocess_conversations(tokenizer, conversaciones, max_length)\n",
        "val_dataset = preprocess_conversations(tokenizer, conversaciones_val, max_length)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Definir el trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "trainer.train()\n",
        "\n",
        "# Paso 4: Guardar el modelo entrenado localmente\n",
        "model.save_pretrained(\"./modelo-ajustado\")\n",
        "tokenizer.save_pretrained(\"./modelo-ajustado\")\n",
        "\n",
        "# Paso 5: Cargar el modelo guardado\n",
        "modelo_ajustado = AutoModelForCausalLM.from_pretrained(\"./modelo-ajustado\")\n",
        "tokenizer_ajustado = AutoTokenizer.from_pretrained(\"./modelo-ajustado\")\n",
        "\n",
        "def interactuar(prompt: str) -> str:\n",
        "  input_ids = tokenizer_ajustado.encode(prompt, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    output = modelo_ajustado.generate(input_ids, min_length=20, max_length=100, pad_token_id=tokenizer_ajustado.eos_token_id)\n",
        "  response = tokenizer_ajustado.decode(output[0], skip_special_tokens=True)\n",
        "  return response\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "i4LPZfY_39xW",
        "outputId": "fb8c789c-e90a-45b3-d91c-ce210168742d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:17, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>12.869484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactuar con el modelo\n",
        "prompt = \"Estoy trabajando en\"\n",
        "respuesta = interactuar(prompt)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fmIaGuROTcb",
        "outputId": "c9736991-013d-48e4-fc3d-2cecc312a4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estoy trabajando en una bendita paz. ¿Qué pasa?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5HuTx-XDO-Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vjq7K9tWYtW3"
      },
      "outputs": [],
      "source": [
        "def plot_comparison(comparison_dict: Dict[str, Tuple[float, float]]) -> None:\n",
        "  names: List[str] = [name for name in comparison_dict.keys()]\n",
        "  losses: List[float] = [loss_tuple[0] for loss_tuple in comparison_dict.values()]\n",
        "  accuracies: List[float] = [math.exp(loss_tuple[0]) for loss_tuple in comparison_dict.values()]\n",
        "\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.scatter(losses, accuracies)\n",
        "  for i, name in enumerate(names):\n",
        "    plt.annotate(name, (losses[i], accuracies[i]))\n",
        "  plt.xlabel(\"Val loss (cross entropy)\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.title(\"Model comparison\")\n",
        "  plt.show()\n",
        "\n",
        "plot_comparison(comparison_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3hgHho8fReo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "token = \"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\"\n",
        "client = InferenceClient(api_key=token)"
      ],
      "metadata": {
        "id": "-hY5-C4lQR7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# este modelo es 90 veces más grande que spanish-dialogpt\n",
        "for message in client.chat_completion(\n",
        "\tmodel=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
        "\tmessages=[{\"role\": \"user\", \"content\": \"I a going to commit suicide\"}],\n",
        "\tmax_tokens=50,\n",
        "\tstream=True,\n",
        "):\n",
        "    print(message.choices[0].delta.content, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIjJ3tDkWPLY",
        "outputId": "ae69d447-3aea-4192-e682-09f20ad57ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm really sorry that you're feeling this way, but I'm here to help. Please tell me what's been troubling you. There are people who care about you and want you to be safe. Here are some resources that can help:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y572KFlUXfcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}