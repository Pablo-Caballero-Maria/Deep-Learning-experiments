{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "MG6caJJKPISz",
        "outputId": "eae3431d-fa1c-46ff-97b1-6fe9087dded7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m777.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.7/347.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.5.0 namex-0.0.8 optree-0.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              },
              "id": "5b514cb434954dac92ca49f18844149b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xg8U6aU7eeb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "from keras import ops\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "INPUT_HEIGHT = 224\n",
        "INPUT_WIDTH = 224\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "MIXED_PRECISION = True\n",
        "SHUFFLE = True\n",
        "\n",
        "# Mixed-precision setting\n",
        "if MIXED_PRECISION:\n",
        "  policy = keras.mixed_precision.Policy(\"mixed_float16\")\n",
        "  keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, valid_ds, test_ds) = tfds.load(\n",
        "  \"oxford_iiit_pet\",\n",
        "  split=[\"train[:85%]\", \"train[85%:]\", \"test\"],\n",
        "  batch_size=BATCH_SIZE,\n",
        "  shuffle_files=SHUFFLE,\n",
        ")"
      ],
      "metadata": {
        "id": "P4e5W6tAIiil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image and Mask Pre-processing\n",
        "# train_ds por ejemplo es un conjunto de objetos \"section\", cada section es un diccionario donde está tanto la image como la máscara\n",
        "def unpack_resize_data(section):\n",
        "  image = section[\"image\"]\n",
        "  segmentation_mask = section[\"segmentation_mask\"]\n",
        "\n",
        "  resize_layer = keras.layers.Resizing(INPUT_HEIGHT, INPUT_WIDTH)\n",
        "\n",
        "  image = resize_layer(image)\n",
        "  segmentation_mask = resize_layer(segmentation_mask)\n",
        "\n",
        "  return image, segmentation_mask\n",
        "\n",
        "# todas las imágenes y máscaras de los 3 conjuntos tendrán las mismas dimensiones (se hace zoom in/zoom out para redimensionar)\n",
        "train_ds = train_ds.map(unpack_resize_data, num_parallel_calls=AUTOTUNE)\n",
        "valid_ds = valid_ds.map(unpack_resize_data, num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(unpack_resize_data, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "6DEFQ7muJMb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, masks = next(iter(test_ds)) # se coge un único batch\n",
        "random_idx = keras.random.uniform([], minval=0, maxval=BATCH_SIZE, seed=10)\n",
        "\n",
        "test_image = images[int(random_idx)].numpy().astype(\"float\")\n",
        "test_mask = masks[int(random_idx)].numpy().astype(\"float\")\n",
        "\n",
        "# Overlay segmentation mask on top of image.\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "\n",
        "ax[0].set_title(\"Image\")\n",
        "ax[0].imshow(test_image / 255.0)\n",
        "\n",
        "ax[1].set_title(\"Image with segmentation mask overlay\")\n",
        "ax[1].imshow(test_image / 255.0)\n",
        "ax[1].imshow(\n",
        "  test_mask,\n",
        "  cmap=\"inferno\",\n",
        "  alpha=0.6,\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tr93Cp_SKVAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dada una imagen y su máscara, se preprocesa normalizando los valores de los píxeles para centrarlos en un rango\n",
        "# lo importante es que sea el rango que sea, todas las imágenes tengan el mismo\n",
        "# \"it will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling\"\n",
        "def preprocess_data(image, segmentation_mask):\n",
        "  image = keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "  return image, segmentation_mask\n",
        "\n",
        "train_ds = (\n",
        "  train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
        "  .shuffle(buffer_size=1024)\n",
        "  .prefetch(buffer_size=1024)\n",
        ")\n",
        "valid_ds = (\n",
        "  valid_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
        "  .shuffle(buffer_size=1024)\n",
        "  .prefetch(buffer_size=1024)\n",
        ")\n",
        "test_ds = (\n",
        "  test_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
        "  .shuffle(buffer_size=1024)\n",
        "  .prefetch(buffer_size=1024)\n",
        ")"
      ],
      "metadata": {
        "id": "Qb4cHKlRLSHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# como backbone se usa VGG19 (se usan los outputs de ciertas capas, osea los features maps)\n",
        "# y luego se le añaden más capas convolucionales propias\n",
        "# capa de entrada de la FCN\n",
        "input_layer = keras.Input(shape=(INPUT_HEIGHT, INPUT_WIDTH, 3))\n",
        "\n",
        "# VGG Model backbone with pre-trained ImageNet weights.\n",
        "# las capas densas que trae include_top serán convertidas en capas convolucionales\n",
        "# una cosa es los filtros de VGG19 que te traes (genéricos) y otra cosa es la importancia de los mismos (los pesos entrenado en este caso con imagenet)\n",
        "vgg_model = keras.applications.vgg19.VGG19(include_top=True, weights=\"imagenet\")\n",
        "\n",
        "# a partir de VGG19 construyo un modelo igual pero más pequeño que reusa el input y algnos outputs\n",
        "fcn_backbone = keras.models.Model(\n",
        "  inputs=vgg_model.layers[1].input,\n",
        "  outputs=[\n",
        "    vgg_model.get_layer(block_name).output\n",
        "    for block_name in [\"block3_pool\", \"block4_pool\", \"block5_pool\"]\n",
        "  ],\n",
        ")\n",
        "\n",
        "fcn_backbone.trainable = False # congela backbone\n",
        "# paso mi input layer a partir del backbone, lo que produce 3 outputs (uno por cada \"ouput\" del backbone) que recojo en \"x\"\n",
        "x = fcn_backbone(input_layer)\n",
        "\n",
        "# el problema de las capas densas es que aplanan la imagen, pero mi capa convolucional no debería hacer eso\n",
        "# una capa densa es como una capa convolucional con un número de filtros igual al número de píxeles de la imagen\n",
        "# las capas convolucionales que creo van a \"imitar\" el comportamiento de las capas densas de vgg19 que había después de block3_pool\", \"block4_pool\", \"block5_pool\n",
        "# vgg funciona bien extrayendo características, eso es reutilizable. pero a la hora de combinar eso en vez de hacerlo con capas densas lo hago con convolucionales\n",
        "# para procesar la info localmente (kernel size). las densas og tenían 4096 uds, osea combis de features extraídas a las que se asignan pesos. incluso con convs\n",
        "# yo puedo generar el mismo número, usando tal num de filters (dado que cada capa aplica los filtros al feature map de la capa anterior, luego esos filtros\n",
        "# buscan \"patrones dentro de patrones\" osea estás generando las mismas combinaciones que lo que hacían las densas que demostraron funcionar bien)\n",
        "units = [4096, 4096]\n",
        "dense_convs = []\n",
        "\n",
        "for filter_idx in range(len(units)): # 0,1\n",
        "  dense_conv = keras.layers.Conv2D(\n",
        "    filters=units[filter_idx], # 4096, 4096 <-- cada filtro busca un patrón, y genera un feature map distinto\n",
        "    kernel_size=(7, 7) if filter_idx == 0 else (1, 1), # (7,7), (1,1)\n",
        "    strides=(1, 1),\n",
        "    activation=\"relu\",\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "    kernel_initializer=keras.initializers.Constant(1.0),\n",
        "  )\n",
        "  dense_convs.append(dense_conv)\n",
        "  dropout_layer = keras.layers.Dropout(0.5)\n",
        "  dense_convs.append(dropout_layer)\n",
        "\n",
        "dense_convs = keras.Sequential(dense_convs) # modelo secuencial construido a partir de la lista de capas convolucionales generada en el bucle anterior\n",
        "dense_convs.trainable = False # congela modelo secuencial\n",
        "\n",
        "x[-1] = dense_convs(x[-1]) # al final del modelo fcn que antes era solo el backbone, métele el resultado de\n",
        "# pasar ese output a través de las capas que acabo de generar\n",
        "\n",
        "pool3_output, pool4_output, pool5_output = x # unwrapea los 3 outputs del backbone"
      ],
      "metadata": {
        "id": "x27c7SpRO9Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esta capa conv sirve para dejar la imagen del tamaño que es (kernel de 1) pero tiene tantos filtros como número de clases\n",
        "# porque al final la salida es \"cuánto se activa cada filtro de clase con esta imagen de entrada\"\n",
        "pool5 = keras.layers.Conv2D(\n",
        "  filters=NUM_CLASSES,\n",
        "  kernel_size=(1, 1),\n",
        "  padding=\"same\",\n",
        "  strides=(1, 1),\n",
        "  activation=\"relu\",\n",
        ")\n",
        "\n",
        "# esta es la capa final que realmente predice las probabilidades de clase, y trabaja sobre la anterior\n",
        "fcn32s_conv_layer = keras.layers.Conv2D(\n",
        "  filters=NUM_CLASSES,\n",
        "  kernel_size=(1, 1),\n",
        "  activation=\"softmax\",\n",
        "  padding=\"same\",\n",
        "  strides=(1, 1),\n",
        ")\n",
        "\n",
        "fcn32s_upsampling = keras.layers.UpSampling2D(\n",
        "  size=(32, 32), # si ves el summary puedes ver cuánto se ha reducido en comparación con la imagen original y por tanto cuánto debes upsamplear\n",
        "  data_format=keras.backend.image_data_format(),\n",
        "  interpolation=\"bilinear\",\n",
        ")\n",
        "\n",
        "final_fcn32s_pool = pool5(pool5_output) # ajustar num canales (salida final del modelo)\n",
        "final_fcn32s_output = fcn32s_conv_layer(final_fcn32s_pool) # (salida def del modelo)\n",
        "final_fcn32s_output = fcn32s_upsampling(final_fcn32s_output)\n",
        "\n",
        "fcn32s_model = keras.Model(inputs=input_layer, outputs=final_fcn32s_output)"
      ],
      "metadata": {
        "id": "glLZ1klIZbSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a parte de mi modelo ya usable \"fcn32s_model\" voy a construir uno que combine la salida de ese modelo con la salida de la penúltima capa\n",
        "# 1x1 convolution to set channels = number of classes\n",
        "# aquí vuelvo a ajustar el número de canales que tiene la salida en este caso de la penúltima capa\n",
        "pool4 = keras.layers.Conv2D(\n",
        "  filters=NUM_CLASSES,\n",
        "  kernel_size=(1, 1),\n",
        "  padding=\"same\",\n",
        "  strides=(1, 1),\n",
        "  activation=\"linear\",\n",
        "  kernel_initializer=keras.initializers.Zeros(),\n",
        ")(pool4_output)\n",
        "\n",
        "# como la salida de final_fcn32s_pool es de 1x1 y la quiero combinar con la salida de pool4 que es 2x2, upsampleo\n",
        "pool5 = keras.layers.UpSampling2D(\n",
        "  size=(2, 2),\n",
        "  data_format=keras.backend.image_data_format(),\n",
        "  interpolation=\"bilinear\",\n",
        ")(final_fcn32s_pool)\n",
        "\n",
        "# capa final de ESTE modelo (predictor)\n",
        "fcn16s_conv_layer = keras.layers.Conv2D(\n",
        "  filters=NUM_CLASSES,\n",
        "  kernel_size=(1, 1),\n",
        "  activation=\"softmax\",\n",
        "  padding=\"same\",\n",
        "  strides=(1, 1),\n",
        ")\n",
        "\n",
        "fcn16s_upsample_layer = keras.layers.UpSampling2D(\n",
        "  size=(16, 16),\n",
        "  data_format=keras.backend.image_data_format(),\n",
        "  interpolation=\"bilinear\",\n",
        ")\n",
        "\n",
        "# combina el output de pool4 con el output (upsampleado) de pool5\n",
        "final_fcn16s_pool = keras.layers.Add()([pool4, pool5])\n",
        "# pásalo por el predictor\n",
        "final_fcn16s_output = fcn16s_conv_layer(final_fcn16s_pool)\n",
        "final_fcn16s_output = fcn16s_upsample_layer(final_fcn16s_output)\n",
        "# esto es OTRO modelo\n",
        "fcn16s_model = keras.models.Model(inputs=input_layer, outputs=final_fcn16s_output)\n"
      ],
      "metadata": {
        "id": "x6AJjl2PmrZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3er y último modelo, se combina el output del modelo fcn16 con el output de pool3\n",
        "# para ello vuelvo a ajustar la profundida del output de la capa 3\n",
        "pool3 = keras.layers.Conv2D(\n",
        "  filters=NUM_CLASSES,\n",
        "  kernel_size=(1, 1),\n",
        "  padding=\"same\",\n",
        "  strides=(1, 1),\n",
        "  activation=\"linear\",\n",
        "  kernel_initializer=keras.initializers.Zeros(),\n",
        ")(pool3_output)\n",
        "\n",
        "# upsampleo la salida del modelo anterior para poder combinarlo\n",
        "intermediate_pool_output = keras.layers.UpSampling2D(\n",
        "  size=(2, 2),\n",
        "  data_format=keras.backend.image_data_format(),\n",
        "  interpolation=\"bilinear\",\n",
        ")(final_fcn16s_pool)\n",
        "\n",
        "# última capa de ESTE modelo (predictor)\n",
        "fcn8s_conv_layer = keras.layers.Conv2D(\n",
        "  filters=NUM_CLASSES,\n",
        "  kernel_size=(1, 1),\n",
        "  activation=\"softmax\",\n",
        "  padding=\"same\",\n",
        "  strides=(1, 1),\n",
        ")\n",
        "\n",
        "fcn8s_upsample_layer = keras.layers.UpSampling2D(\n",
        "  size=(8, 8),\n",
        "  data_format=keras.backend.image_data_format(),\n",
        "  interpolation=\"bilinear\",\n",
        ")\n",
        "\n",
        "# ahora combino esas 2\n",
        "final_fcn8s_pool = keras.layers.Add()([pool3, intermediate_pool_output])\n",
        "final_fcn8s_output = fcn8s_conv_layer(final_fcn8s_pool)\n",
        "final_fcn8s_output = fcn8s_upsample_layer(final_fcn8s_output)\n",
        "\n",
        "fcn8s_model = keras.models.Model(inputs=input_layer, outputs=final_fcn8s_output)"
      ],
      "metadata": {
        "id": "hlSZVeV9oZq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esto es para reutilizar los pesos de las últimas capas densas de vgg, para construir una versión de estas capas como capas convolucionales\n",
        "weights1 = vgg_model.get_layer(\"fc1\").get_weights()[0]\n",
        "weights2 = vgg_model.get_layer(\"fc2\").get_weights()[0]\n",
        "\n",
        "# las 2 últimas capas (convolucionales) de mi modelo tienen esta forma, osea son como las densas de vgg y por eso funciona bien que se le metan esos pesos\n",
        "weights1 = weights1.reshape(7, 7, 512, 4096)\n",
        "weights2 = weights2.reshape(1, 1, 4096, 4096)\n",
        "\n",
        "dense_convs.layers[0].set_weights([weights1])\n",
        "dense_convs.layers[2].set_weights([weights2])"
      ],
      "metadata": {
        "id": "fr6d4ozipp7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fcn16s_optimizer = keras.optimizers.AdamW(\n",
        "  learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "fcn16s_loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "fcn16s_model.compile(\n",
        "  optimizer=fcn16s_optimizer,\n",
        "  loss=fcn16s_loss,\n",
        "  metrics=[\n",
        "    keras.metrics.MeanIoU(num_classes=NUM_CLASSES, sparse_y_pred=False),\n",
        "    keras.metrics.SparseCategoricalAccuracy(),\n",
        "  ],\n",
        ")\n",
        "\n",
        "fcn16s_history = fcn16s_model.fit(train_ds, epochs=EPOCHS, validation_data=valid_ds)"
      ],
      "metadata": {
        "id": "jCkrfyYNrD0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, masks = next(iter(test_ds))\n",
        "random_idx = keras.random.uniform([], minval=0, maxval=BATCH_SIZE,seed=10)\n",
        "\n",
        "# Get random test image and mask\n",
        "test_image = images[int(random_idx)].numpy().astype(\"float\")\n",
        "test_mask = masks[int(random_idx)].numpy().astype(\"float\")\n",
        "\n",
        "pred_image = ops.expand_dims(test_image, axis=0)\n",
        "pred_image = keras.applications.vgg19.preprocess_input(pred_image)\n",
        "\n",
        "# Perform inference on FCN-16S\n",
        "pred_mask_16s = fcn16s_model.predict(pred_image, verbose=0).astype(\"float\")\n",
        "pred_mask_16s = np.argmax(pred_mask_16s, axis=-1)\n",
        "pred_mask_16s = pred_mask_16s[0, ...]\n",
        "\n",
        "# Plot all results\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 8))\n",
        "\n",
        "fig.delaxes(ax[0, 2])\n",
        "\n",
        "ax[0, 0].set_title(\"Image\")\n",
        "ax[0, 0].imshow(test_image / 255.0)\n",
        "\n",
        "ax[0, 1].set_title(\"Image with ground truth overlay\")\n",
        "ax[0, 1].imshow(test_image / 255.0)\n",
        "ax[0, 1].imshow(\n",
        "    test_mask,\n",
        "    cmap=\"inferno\",\n",
        "    alpha=0.6,\n",
        ")\n",
        "\n",
        "ax[1, 1].set_title(\"Image with FCN-16S mask overlay\")\n",
        "ax[1, 1].imshow(test_image / 255.0)\n",
        "ax[1, 1].imshow(pred_mask_16s, cmap=\"inferno\", alpha=0.6)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "txD0HcXAtA9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}